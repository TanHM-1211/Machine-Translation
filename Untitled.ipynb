{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tokenizer.BPE import BPE_VI, BPE_EN\n",
    "from tokenizer.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE_EN(padding=False)\n",
    "\n",
    "tokenizer = Tokenizer(bpe.symbols, bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, device='cuda', embedding=None, embedding_dim=128, padding_idx=0):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = 100\n",
    "        self.lstm_dim = 256\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_dim = 128\n",
    "        self.output_dim = self.vocab_size\n",
    "        self.bos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "\n",
    "        if embedding is not None:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embedding, padding_idx=padding_idx)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
    "        self.direction = 2\n",
    "        self.encoder = nn.LSTM(self.embedding_dim, self.lstm_dim, batch_first=True, bidirectional=True, num_layers=1)\n",
    "        self.decoder = nn.LSTM(self.embedding_dim, self.lstm_dim, batch_first=True, bidirectional=True, num_layers=1)\n",
    "        self.linear = nn.Linear(self.lstm_dim * self.direction, self.output_dim)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.loss_ignore_idx = -100\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=self.loss_ignore_idx)\n",
    "\n",
    "        self.beam_size = 5\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x: List[torch.LongTensor], y:  List[torch.LongTensor] = None, max_len=20, beam_size=None):\n",
    "\n",
    "        lens = [len(sent) for sent in x]\n",
    "\n",
    "        # padding\n",
    "        x = pad_sequence(x, batch_first=True, padding_value=self.embeddings.padding_idx).to(self.device)\n",
    "        x = self.embeddings(x)  # shape: batch * max(lens) * embedding_dim\n",
    "\n",
    "        # packing\n",
    "        x = pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # forward\n",
    "        out_packed, (h, c) = self.encoder(x)\n",
    "        print(out_packed)\n",
    "        # out, out_lens = pad_packed_sequence(out_packed, batch_first=True)\n",
    "        if y is not None:\n",
    "            decoder_inputs = [sent[:-1].clone() for sent in y]\n",
    "            decoder_outputs = [sent[1:].clone() for sent in y]\n",
    "\n",
    "            decoder_inputs_lens = [len(sent) for sent in decoder_inputs]\n",
    "            decoder_inputs = pad_sequence(decoder_inputs, batch_first=True, padding_value=self.embeddings.padding_idx)\n",
    "            decoder_inputs = self.embeddings(decoder_inputs)\n",
    "            decoder_inputs = pack_padded_sequence(decoder_inputs, decoder_inputs_lens, batch_first=True,\n",
    "                                                  enforce_sorted=False)\n",
    "\n",
    "            decoder_outputs = pad_sequence(decoder_outputs, batch_first=True, padding_value=self.loss_ignore_idx)\n",
    "            out_packed, (h, c) = self.decoder(decoder_inputs, (h, c))\n",
    "            # unpack\n",
    "            out, lens_unpack = pad_packed_sequence(out_packed, batch_first=True,\n",
    "                                                   padding_value=self.embeddings.padding_idx)\n",
    "            # linear forward\n",
    "            print(out.shape)\n",
    "            out = self.linear(out)\n",
    "            loss = self.loss(out, decoder_outputs)\n",
    "            return loss\n",
    "        else:\n",
    "            # h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "            if beam_size is None:\n",
    "                beam_size = beam_size\n",
    "            res = []\n",
    "            for batch_i in range(h.shape[1]):\n",
    "                h_i = h[:, batch_i: batch_i + 1, :]\n",
    "                c_i = c[:, batch_i: batch_i + 1, :]\n",
    "                print(batch_i)\n",
    "                res.append(self.forward_sent((h_i.contiguous(), c_i.contiguous()), max_len=max_len, beam_size=beam_size))\n",
    "            return res\n",
    "\n",
    "    # def forward_sent(self, states, max_len=200, beam_size=5):\n",
    "    #     # h, c = states\n",
    "    #     temp_input = torch.LongTensor([self.bos_idx]).unsqueeze(0).to(self.device)\n",
    "    #     res = []\n",
    "    #     while True:\n",
    "    #         temp_output, states = self.decoder(self.embeddings(temp_input), states)\n",
    "    #         temp_output_idx = torch.argmax(self.softmax(temp_output.squeeze(0)), dim=-1)\n",
    "    #         res.append(temp_output_idx)\n",
    "    #         if temp_output_idx == self.eos_idx or len(res) == max_len:\n",
    "    #             break\n",
    "    #         temp_input = torch.LongTensor([temp_output_idx]).unsqueeze(0).to(self.device)\n",
    "    #\n",
    "    #     return torch.LongTensor(res).to(self.device)\n",
    "\n",
    "    def normalize_prob(self, prob):\n",
    "        return np.log(prob)\n",
    "\n",
    "    def forward_one_token(self, token, states, beam_size=None):\n",
    "        if beam_size is None:\n",
    "            beam_size = self.beam_size\n",
    "        input_id = torch.LongTensor([[token]]).to(self.device)\n",
    "        output, states = self.decoder(self.embeddings(input_id), states)\n",
    "        topk_output = torch.topk(self.softmax(output.squeeze(0).squeeze(0)), k=beam_size, dim=-1)\n",
    "        topk_output_indices = topk_output.indices.tolist()  # for next token\n",
    "        topk_output_values = topk_output.values.tolist()  # for probability of next token\n",
    "\n",
    "        return topk_output_indices, topk_output_values, states\n",
    "\n",
    "    def forward_sent(self, states, max_len=50, beam_size=None):\n",
    "        # h, c = states\n",
    "        if beam_size is None:\n",
    "            beam_size = self.beam_size\n",
    "\n",
    "        # initialize\n",
    "        topk_output_indices, topk_output_values, states = self.forward_one_token(self.bos_idx, states, beam_size)\n",
    "        res = []\n",
    "        for i in range(len(topk_output_indices)):\n",
    "            res.append([[topk_output_indices[i]], self.normalize_prob(topk_output_values[i]), states])\n",
    "\n",
    "        while True:\n",
    "            candidates = []\n",
    "            count_eos_token = 0\n",
    "            for input_ids, accumulate_prob, states in res:\n",
    "                input_id = input_ids[-1]\n",
    "                if input_id != self.eos_idx and len(input_ids) < max_len - 1:\n",
    "                    topk_output_indices, topk_output_values, new_states = self.forward_one_token(self.bos_idx, states,\n",
    "                                                                                                 beam_size)\n",
    "                    for i in range(len(topk_output_indices)):\n",
    "                        candidates.append([input_ids + [topk_output_indices[i]],\n",
    "                                           (accumulate_prob * len(input_ids) +\n",
    "                                            self.normalize_prob(topk_output_values[i])) / (len(input_ids)+1),  # normalize with len\n",
    "                                           new_states])\n",
    "                elif input_id == self.eos_idx:\n",
    "                    count_eos_token += 1\n",
    "                else:\n",
    "                    input_ids.append(self.eos_idx)\n",
    "            if count_eos_token == beam_size:\n",
    "                break\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            res = candidates[:min(beam_size, len(candidates))]\n",
    "\n",
    "        return torch.LongTensor(res[0][0]).to(self.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([   0, 1708, 8382,  652,   24,   35,  497,    5, 2731,    9,   42,  516,\n",
      "           9, 2053,  965,   90, 1078,  480,   63, 2099,    2]), tensor([    0, 46202,  1550,    19,  4205,  3260,   321,     2])]\n",
      "PackedSequence(data=tensor([[-0.0126, -0.0024, -0.0128,  ...,  0.1108, -0.0450, -0.0891],\n",
      "        [-0.0126, -0.0024, -0.0128,  ...,  0.0231, -0.1003,  0.0209],\n",
      "        [ 0.0414,  0.0289, -0.0088,  ...,  0.1983, -0.0400, -0.1358],\n",
      "        ...,\n",
      "        [ 0.0927,  0.2422,  0.0833,  ...,  0.1197, -0.1648,  0.0546],\n",
      "        [ 0.0210,  0.4060, -0.1209,  ..., -0.0735, -0.0407,  0.0940],\n",
      "        [ 0.0753,  0.2382, -0.1235,  ...,  0.0442, -0.0486,  0.1644]],\n",
      "       device='cuda:0', grad_fn=<CudnnRnnBackward>), batch_sizes=tensor([2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=tensor([0, 1], device='cuda:0'), unsorted_indices=tensor([0, 1], device='cuda:0'))\n",
      "0\n",
      "[48, 87, 192, 196, 237]\n",
      "[48, 87, 192, 196, 237]\n",
      "[48, 87, 192, 196, 237]\n",
      "[48, 87, 192, 196, 237]\n",
      "[48, 87, 192, 196, 237]\n",
      "[192, 87, 237, 48, 196]\n",
      "[192, 87, 237, 48, 196]\n",
      "[192, 87, 237, 48, 196]\n",
      "[192, 87, 237, 48, 196]\n",
      "[192, 87, 237, 48, 196]\n",
      "[108, 192, 237, 87, 92]\n",
      "[108, 192, 237, 87, 92]\n",
      "[108, 192, 237, 87, 92]\n",
      "[108, 192, 237, 87, 92]\n",
      "[108, 192, 237, 87, 92]\n",
      "[108, 192, 237, 92, 87]\n",
      "[108, 192, 237, 92, 87]\n",
      "[108, 192, 237, 92, 87]\n",
      "[108, 192, 237, 92, 87]\n",
      "[108, 192, 237, 92, 87]\n",
      "[108, 237, 192, 92, 87]\n",
      "[108, 237, 192, 92, 87]\n",
      "[108, 237, 192, 92, 87]\n",
      "[108, 237, 192, 92, 87]\n",
      "[108, 237, 192, 92, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n",
      "[108, 92, 237, 192, 87]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-18156d659647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_generated_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# print(tokenizer.merge(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-179679aa2bb8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, y, max_len, beam_size)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mc_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-179679aa2bb8>\u001b[0m in \u001b[0;36mforward_sent\u001b[1;34m(self, states, max_len, beam_size)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mcandidates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "vocab_size = len(bpe.symbols)\n",
    "model = Seq2SeqModel(vocab_size=vocab_size, device=device)\n",
    "model.to(device)\n",
    "\n",
    "test_len = 10\n",
    "max_generated_len = 30\n",
    "\n",
    "# s = ['Bánh rán là một món ăn ưa thích của Doraemon', 'Có một vài LeeSin đá sóng âm hụt']\n",
    "# tokenizer = BPE_VI(padding=False)\n",
    "s = ['But lets face it: At the core of this line of thinking isnt safety -- its sex', 'Process finished with exit code 0']\n",
    "\n",
    "x = tokenizer.tokenize(s)\n",
    "print(x)\n",
    "print(tokenizer.merge([i.numpy() for i in model(x, max_len=max_generated_len)]))\n",
    "\n",
    "# print(tokenizer.merge(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
